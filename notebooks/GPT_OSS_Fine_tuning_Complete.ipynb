{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e350424a",
   "metadata": {},
   "source": [
    "# GPT-OSS Fine-tuning with Unsloth and Bright Data\n",
    "Complete notebook for fine-tuning GPT-OSS-20B using Unsloth and Bright Data.\n",
    "Run this on Google Colab with a T4 GPU (free tier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth and dependencies\n",
    "import subprocess, sys\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '--upgrade', '-q', 'uv'])\n",
    "try:\n",
    "    import numpy\n",
    "    get_numpy = f'numpy=={numpy.__version__}'\n",
    "except:\n",
    "    get_numpy = 'numpy'\n",
    "packages = [\n",
    "    'torch>=2.8.0',\n",
    "    'triton>=3.4.0',\n",
    "    get_numpy,\n",
    "    'torchvision',\n",
    "    'bitsandbytes',\n",
    "    'transformers>=4.55.3',\n",
    "    'unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo',\n",
    "    'unsloth[base] @ git+https://github.com/unslothai/unsloth',\n",
    "    'git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels'\n",
    "]\n",
    "import os\n",
    "os.system(f\"uv pip install -qqq {' '.join(packages)}\")\n",
    "os.system('uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers')\n",
    "os.system('uv pip install --no-deps trl==0.22.2')\n",
    "os.system('pip install -q brightdata-sdk')\n",
    "print('‚úÖ Dependencies installed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and import libraries\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only\n",
    "from datasets import Dataset, load_dataset\n",
    "from typing import List, Dict\n",
    "import time\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f'GPU = {gpu_stats.name}. Max memory = {max_memory} GB.')\n",
    "if max_memory < 15:\n",
    "    print('‚ö†Ô∏è Warning: You need at least 16GB GPU memory. Switch to T4 or better.')\n",
    "else:\n",
    "    print('‚úÖ GPU memory sufficient for GPT-OSS-20B fine-tuning!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f33b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-OSS-20B Model with Unsloth\n",
    "max_seq_length = 1024\n",
    "dtype = None\n",
    "print('Loading GPT-OSS-20B with Unsloth...')\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = 'unsloth/gpt-oss-20b',\n",
    "    dtype = dtype,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    "    full_finetuning = False,\n",
    ")\n",
    "print('‚úÖ Model loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b537b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA Adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8,\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj',\n",
    "                      'gate_proj', 'up_proj', 'down_proj'],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = 'none',\n",
    "    use_gradient_checkpointing = 'unsloth',\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "print('‚úÖ LoRA applied!')\n",
    "print(f'Training {trainable_params:,} / {all_params:,} params')\n",
    "print(f\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66351f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Reasoning Effort Levels\n",
    "print('Testing reasoning effort levels...')\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is 15 * 23?\"}]\n",
    "print('=== LOW REASONING (Fast) ===')\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt', return_dict=True, reasoning_effort='low').to('cuda')\n",
    "_ = model.generate(**inputs, max_new_tokens=32, streamer=TextStreamer(tokenizer))\n",
    "print('=== HIGH REASONING (Accurate) ===')\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt', return_dict=True, reasoning_effort='high').to('cuda')\n",
    "_ = model.generate(**inputs, max_new_tokens=128, streamer=TextStreamer(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Training Data with Bright Data\n",
    "from brightdata import bdclient\n",
    "from typing import List, Dict\n",
    "import re\n",
    "class DataCollector:\n",
    "    def __init__(self, api_token: str):\n",
    "        self.client = bdclient(api_token=api_token)\n",
    "        self.collected_data = []\n",
    "        print('‚úÖ Bright Data client initialized')\n",
    "    def collect_documentation(self, urls: List[str]) -> List[Dict]:\n",
    "        print(f'Scraping {len(urls)} URLs with Bright Data...')\n",
    "        try:\n",
    "            results = self.client.scrape(urls, data_format='markdown')\n",
    "            if isinstance(results, str):\n",
    "                training_data = self.process_single_result(results)\n",
    "            elif isinstance(results, list):\n",
    "                training_data = []\n",
    "                for content in results:\n",
    "                    if content:\n",
    "                        examples = self.process_single_result(content)\n",
    "                        training_data.extend(examples)\n",
    "            else:\n",
    "                print(f'Unexpected result type: {type(results)}')\n",
    "                training_data = []\n",
    "        except Exception as e:\n",
    "            print(f'Batch scraping failed: {e}')\n",
    "            training_data = []\n",
    "            for url in urls:\n",
    "                try:\n",
    "                    print(f'  Scraping: {url}')\n",
    "                    content = self.client.scrape(url, data_format='markdown')\n",
    "                    if content:\n",
    "                        examples = self.process_single_result(content)\n",
    "                        training_data.extend(examples)\n",
    "                        print(f'    ‚úì Got {len(examples)} examples')\n",
    "                except Exception as url_error:\n",
    "                    print(f'    ‚úó Error: {url_error}')\n",
    "        self.collected_data = training_data\n",
    "        print(f'‚úÖ Total examples collected: {len(self.collected_data)}')\n",
    "        return self.collected_data\n",
    "    def process_single_result(self, content: str) -> List[Dict]:\n",
    "        examples = []\n",
    "        content = re.sub(r'<[^>]+>', '', content)\n",
    "        content = re.sub(r'!.*?.*?', '', content)\n",
    "        content = re.sub(r'([^]+)[^]+', r'', content)\n",
    "        content = re.sub(r'```[^`]*```', '', content)\n",
    "        content = re.sub(r'`[^`]+`', '', content)\n",
    "        content = re.sub(r'[#*_~>`|-]+', ' ', content)\n",
    "        content = re.sub(r'\\(.)', r'', content)\n",
    "        content = re.sub(r'https?://[^]+', '', content)\n",
    "        content = re.sub(r'++', '', content)\n",
    "        content = re.sub(r'+', ' ', content)\n",
    "        sentences = re.split(r'(?<=[.!?])+', content)\n",
    "        clean_sentences = []\n",
    "        for sent in sentences:\n",
    "            sent = sent.strip()\n",
    "            if (len(sent) > 30 and not any(skip in sent.lower() for skip in ['navigation', 'copyright', 'index', 'table of contents', 'previous', 'next'])):\n",
    "                clean_sentences.append(sent)\n",
    "        for i in range(0, len(clean_sentences) - 1):\n",
    "            instruction = clean_sentences[i][:200].strip()\n",
    "            response = clean_sentences[i + 1][:300].strip()\n",
    "            if len(instruction) > 20 and len(response) > 30:\n",
    "                examples.append({'instruction': instruction, 'response': response})\n",
    "        return examples\n",
    "BRIGHTDATA_API_TOKEN = 'your_token_here'\n",
    "urls = [\n",
    "    'https://docs.python.org/3/tutorial/introduction.html',\n",
    "    'https://docs.python.org/3/tutorial/controlflow.html',\n",
    "    'https://docs.python.org/3/tutorial/datastructures.html',\n",
    "]\n",
    "collector = DataCollector(api_token=BRIGHTDATA_API_TOKEN)\n",
    "print('Collecting training data...')\n",
    "training_data = collector.collect_documentation(urls)\n",
    "if len(training_data) == 0:\n",
    "    print('‚ö†Ô∏è ERROR: No training data collected!')\n",
    "    raise ValueError('No training data collected')\n",
    "# Final validation\n",
    "def final_validation(examples: List[Dict]) -> List[Dict]:\n",
    "    clean_data = []\n",
    "    seen = set()\n",
    "    for ex in examples:\n",
    "        instruction = ex.get('instruction', '').strip()\n",
    "        response = ex.get('response', '').strip()\n",
    "        instruction = re.sub(r'[^a-zA-Z0-9,?!]', '', instruction)\n",
    "        response = re.sub(r'[^a-zA-Z0-9,?!]', '', response)\n",
    "        if (len(instruction) > 10 and len(response) > 20 and instruction not in seen):\n",
    "            seen.add(instruction)\n",
    "            clean_data.append({'instruction': instruction, 'response': response})\n",
    "    return clean_data\n",
    "training_data = final_validation(training_data)\n",
    "print(f'Final clean dataset: {len(training_data)} examples')\n",
    "if len(training_data) == 0:\n",
    "    raise ValueError('No valid training data after cleaning')\n",
    "print('Clean training examples:')\n",
    "for i, example in enumerate(training_data[:3]):\n",
    "    print(f'Example {i+1}:')\n",
    "    print(f'Instruction: {example[\"instruction\"]}')\n",
    "    print(f'Response: {example[\"response\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f422218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Data for Training\n",
    "from datasets import Dataset\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "def prepare_dataset(raw_data):\n",
    "    formatted_data = []\n",
    "    for item in raw_data:\n",
    "        formatted_data.append({\n",
    "            'messages': [\n",
    "                {'role': 'user', 'content': item['instruction']},\n",
    "                {'role': 'assistant', 'content': item['response']}\n",
    "            ]\n",
    "        })\n",
    "    dataset = Dataset.from_list(formatted_data)\n",
    "    dataset = standardize_sharegpt(dataset)\n",
    "    def formatting_prompts_func(examples):\n",
    "        convos = examples['messages']\n",
    "        texts = []\n",
    "        for convo in convos:\n",
    "            text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "        return {'text': texts}\n",
    "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "    print(f'‚úÖ Dataset ready with {len(dataset)} examples')\n",
    "    print('Example formatted text (first 500 chars):')\n",
    "    print(dataset[0]['text'][:500])\n",
    "    if '<|channel|>' not in dataset[0]['text']:\n",
    "        print('‚ö†Ô∏è Warning: Missing channel in format. Adding explicit channel...')\n",
    "        def fix_formatting(examples):\n",
    "            fixed_texts = []\n",
    "            for text in examples['text']:\n",
    "                text = text.replace('<|start|>assistant<|message|>', '<|start|>assistant<|channel|>final<|message|>')\n",
    "                fixed_texts.append(text)\n",
    "            return {'text': fixed_texts}\n",
    "        dataset = dataset.map(fix_formatting, batched=True)\n",
    "        print('‚úÖ Fixed formatting with channel')\n",
    "        print('Fixed example (first 500 chars):')\n",
    "        print(dataset[0]['text'][:500])\n",
    "    return dataset\n",
    "dataset = prepare_dataset(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Training Configuration\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = 'adamw_8bit',\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = 'linear',\n",
    "        seed = 3407,\n",
    "        output_dir = 'outputs',\n",
    "        report_to = 'none',\n",
    "    ),\n",
    ")\n",
    "gpt_oss_kwargs = dict(\n",
    "    instruction_part = '<|start|>user<|message|>',\n",
    "    response_part = '<|start|>assistant<|channel|>final<|message|>'\n",
    ")\n",
    "trainer = train_on_responses_only(trainer, **gpt_oss_kwargs)\n",
    "print('‚úÖ Trainer configured!')\n",
    "sample = trainer.train_dataset[0]\n",
    "decoded_labels = tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in sample['labels']]).replace(tokenizer.pad_token, ' ')\n",
    "print('Verifying we only train on assistant responses:')\n",
    "print(f'Training on: {decoded_labels[:200]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24312023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f'GPU memory reserved before training: {start_gpu_memory} GB')\n",
    "print('üöÄ Starting training...')\n",
    "print('This will take about 5-10 minutes for 30 steps.')\n",
    "print('For full training, set max_steps=None and num_train_epochs=1')\n",
    "trainer_stats = trainer.train()\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "print('‚úÖ Training completed!')\n",
    "print(f'Time: {trainer_stats.metrics['train_runtime']/60:.1f} minutes')\n",
    "print(f'Final loss: {trainer_stats.metrics['train_loss']:.4f}')\n",
    "print(f'Peak memory for training: {used_memory_for_lora} GB')\n",
    "print(f'Total peak memory: {used_memory} GB / {max_memory} GB ({used_memory/max_memory*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Fine-tuned Model\n",
    "print('üß™ Testing fine-tuned model on Python questions...')\n",
    "def test_model(prompt: str, reasoning_effort: str = 'medium'):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a helpful Python expert assistant.'},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors='pt', return_dict=True, reasoning_effort=reasoning_effort).to('cuda')\n",
    "    print(f'Question: {prompt}')\n",
    "    print(f'Reasoning effort: {reasoning_effort}')\n",
    "    print('Answer:')\n",
    "    outputs = model.generate(**inputs, max_new_tokens=150, streamer=TextStreamer(tokenizer, skip_prompt=True), temperature=0.7, top_p=0.9)\n",
    "    print('' + '='*60 + '')\n",
    "test_questions = [\n",
    "    'What is a Python generator?',\n",
    "    'How do I read a CSV file in Python?',\n",
    "    'Explain async/await in Python'\n",
    "]\n",
    "for question in test_questions:\n",
    "    print(f'{'='*60}')\n",
    "    print(f'Question: {question}')\n",
    "    print(f'')\n",
    "    response = test_model(question, reasoning_effort='medium')\n",
    "complex_question = 'Write a Python function that finds all prime numbers up to n using the Sieve of Eratosthenes'\n",
    "print('='*60)\n",
    "print('TESTING REASONING EFFORT LEVELS')\n",
    "print('='*60)\n",
    "for effort in ['low', 'medium', 'high']:\n",
    "    print(f'{'='*40}')\n",
    "    print(f'Reasoning Effort: {effort.upper()}')\n",
    "    print(f'')\n",
    "    _ = test_model(complex_question, reasoning_effort=effort)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a04007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Fine-tuned Model\n",
    "print('üíæ Saving model...')\n",
    "model.save_pretrained('gpt-oss-python-expert-lora')\n",
    "tokenizer.save_pretrained('gpt-oss-python-expert-lora')\n",
    "print('‚úÖ LoRA adapters saved to gpt-oss-python-expert-lora')\n",
    "# Optional: Push to Hugging Face Hub\n",
    "# model.push_to_hub('dexcodes/gpt-oss-python-expert-lora', token='your_hf_token')\n",
    "# model.push_to_hub_merged('dexcodes/gpt-oss-python-expert', tokenizer, save_method='mxfp4', token='your_hf_token')\n",
    "print('‚úÖ Model pushed to Hugging Face Hub!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd0f1b",
   "metadata": {},
   "source": [
    "üéâ Congratulations! You've successfully:\n",
    "\n",
    "1. ‚úÖ Loaded GPT-OSS-20B with Unsloth (2x faster than vanilla transformers)\n",
    "2. ‚úÖ Applied LoRA for efficient training (only 1% of parameters)\n",
    "3. ‚úÖ Collected training data (would use Bright Data in production)\n",
    "4. ‚úÖ Fine-tuned the model on Python Q&A\n",
    "5. ‚úÖ Tested the model with different reasoning efforts\n",
    "6. ‚úÖ Saved the fine-tuned model\n",
    "\n",
    "Next steps:\n",
    "- Use Bright Data API for real web scraping: https://brightdata.com\n",
    "- Train for longer (set num_train_epochs=1)\n",
    "- Try GPT-OSS-120B if you have A100 GPU\n",
    "- Deploy with vLLM for production: https://github.com/vllm-project/vllm\n",
    "- Join Unsloth Discord for tips: https://discord.gg/unsloth\n",
    "\n",
    "Resources:\n",
    "- Unsloth Docs: https://docs.unsloth.ai/\n",
    "- Bright Data: https://brightdata.com/solutions/ai\n",
    "- Original notebook: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
